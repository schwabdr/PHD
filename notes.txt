################## INFORMATION ##################
I see now that they define a special forward function for resnet_new.py
They can accept which layer's output they want to return ... and therefore they use this model
as the encoder. This will help me understand the paper a bit better now. The encoder is one of the outer layers
of the resnet model.

https://discuss.pytorch.org/t/how-can-you-train-your-model-on-large-batches-when-your-gpu-can-t-hold-more-than-a-few-samples/80581
https://discuss.pytorch.org/t/how-are-batches-split-across-multiple-gpus/91225


################## CURRENT WORK ##################
1) TRAIN RESNET18 FOR CIFAR 10 - NO AT - DONE
2) Make train_MI_estimator.py more efficient. - DONE
3) See Adversarial Training in "Adversarial Machine Learning at Scale" apply here

************ remove the eval step from the training of the MIAT - don't need it really unless I'm going to plot, and it will speed up training *************
************ optimize the PGD That is don't keep iterating if it's already fooling the target***************
Put a cap on iterations in my "eval_tests_01.py"

################## NORMALIZATION ##################
https://github.com/ernoult/scalingDTP/pull/36
And also see:
https://github.com/kuangliu/pytorch-cifar/issues/19

################## CONDA ENVIRONMENT ##################
conda env phd04 - here's what I've done - pytorch 1.12 - I hope this keeps working.

conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
conda install matplotlib
conda install -c conda-forge torchinfo






################## CODE SNIPPETS ##################
img, label = trainset.__getitem__(0)
    min = torch.min(img)
    max = torch.max(img)
    for i in range(trainset.__len__()):
        img, label = trainset.__getitem__(i)
        min_i = torch.min(img)
        max_i = torch.max(img)
        if min_i < min:
            min = min_i
        if max_i > max:
            max = max_i
    print(f"min: {torch.min(img)}")
    print(f"max: {torch.max(img)}")